{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a"
   },
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "import tensorflow.keras.utils as ku \n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making data ready to train:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = open('artists/bruno-mars.txt').read()\n",
    "\n",
    "tokenizer = Tokenizer()\n",
    "\n",
    "corpus = data.lower().split(\"\\n\")\n",
    "\n",
    "\n",
    "tokenizer.fit_on_texts(corpus)\n",
    "total_words = len(tokenizer.word_index) + 1  #adding one for out-of-vocab words\n",
    "\n",
    "# creating input sequences using list of tokens\n",
    "input_sequences = []\n",
    "for l in corpus:\n",
    "\ttoken_list = tokenizer.texts_to_sequences([l])[0]\n",
    "\tfor i in range(1, len(token_list)):\n",
    "\t\tn_gram_seq = token_list[:i+1]\n",
    "\t\tinput_sequences.append(n_gram_seq)\n",
    "\n",
    "\n",
    "# implementing padding\n",
    "max_seq_len = max([len(x) for x in input_sequences])\n",
    "input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_seq_len, padding='pre'))\n",
    "\n",
    "predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n",
    "\n",
    "# one hot code\n",
    "label = ku.to_categorical(label, num_classes=total_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model= Sequential([\n",
    "    Embedding(total_words, 100, input_length=max_sequence_len-1), # -1 as the last word is the label\n",
    "    Bidirectional(LSTM(150, return_sequences = True)),\n",
    "    Dropout(0.2),  #drops 20% of units in a layer\n",
    "    LSTM(100),\n",
    "    Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)),\n",
    "    Dense(total_words, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs= 100\n",
    "history = model.fit(predictors, label, epochs=num_epochs, verbose=1)\n",
    "\n",
    "# LAST EPOCH I GOT:\n",
    "#Epoch 100/100\n",
    "#739/739 [==============================] - 29s 39ms/step - loss: 1.0005 - accuracy: 0.8041"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# To visualize accuracy and loss over the epochs trained:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "accuracy = history.history['accuracy']\n",
    "loss = history.history['loss']\n",
    "\n",
    "epochs = range(len(accuracy))\n",
    "\n",
    "plt.plot(epochs, accuracy, 'r', label='Training accuracy')\n",
    "plt.title('Training accuracy')\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.plot(epochs, loss, 'r', label='Training Loss')\n",
    "plt.title('Training loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating the lyrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_text = \"You know what I want\"\n",
    "num_words = 60  #number of words to generate\n",
    "  \n",
    "for _ in range(next_words):\n",
    "\ttoken_list = tokenizer.texts_to_sequences([start_text])[0]\n",
    "\ttoken_list = pad_sequences([token_list], maxlen=max_seq_len-1, padding='pre')\n",
    "\tpredicted = model.predict_classes(token_list, verbose=0)\n",
    "\tout_word = \"\"\n",
    "\tfor word, index in tokenizer.word_index.items():\n",
    "\t\tif index == predicted:\n",
    "\t\t\tout_word = word\n",
    "\t\t\tbreak\n",
    "\tstart_text += \" \" + out_word\n",
    "print(start_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**OUTPUT I RECEIVED:**\n",
    "You know what I want you \n",
    "when you're looking for no replacement \n",
    "now i'm madly in love with you \n",
    "i know that you're waiting for \n",
    "more get good for her in their way \n",
    "oh i love you more today than yesterday \n",
    "tonight i wanna jump in the cadillac \n",
    "is the night is that \n",
    "i will break these chains \n",
    "that bind me \n",
    "happiness will find me"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
